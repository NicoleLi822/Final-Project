{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a89b7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffb492",
   "metadata": {},
   "source": [
    "## Part1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a0046",
   "metadata": {},
   "source": [
    "## distance calculation\n",
    "Define a function to caculate distcance between two pairs of given longtitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af3b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to calculate distance\n",
    "def create_distance(lat1,lon1,lat2,lon2):\n",
    "    dist = geodesic((lat1,lon1),(lat2,lon2)).km\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa72864",
   "metadata": {},
   "source": [
    "## uber_date\n",
    "1. Import and go through the data, delete the data out of given range\n",
    "2. Crate a 'year' column to obatin the # of datapoints in ervery year\n",
    "3. Drop invalid variables and add distance to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b659511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import uber_data\n",
    "uber = pd.read_csv('uber_rides_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903ad794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06.0000003</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          key  fare_amount  \\\n",
       "0    24238194  2015-05-07 19:52:06.0000003          7.5   \n",
       "1    27835199  2009-07-17 20:04:56.0000002          7.7   \n",
       "\n",
       "           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0         -73.999512         40.723217                1  \n",
       "1         -73.994710         40.750325                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "uber.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6761298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep data in required range\n",
    "uber_select = uber[(uber['pickup_longitude'] >= -74.242330) & (uber['pickup_longitude'] <= -73.717047) \\\n",
    "    & (uber['pickup_latitude'] >= 40.560445) & (uber['pickup_latitude'] <= 40.908524) &\\\n",
    "    (uber['dropoff_longitude'] >= -74.242330) & (uber['dropoff_longitude'] <= -73.717047) \\\n",
    "    & (uber['dropoff_latitude'] >= 40.560445) & (uber['dropoff_latitude'] <= 40.908524)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2c2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-77b182171db7>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uber_select['year'] = pd.to_datetime(uber['pickup_datetime']).dt.year\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2012    31531\n",
       "2011    31102\n",
       "2013    30496\n",
       "2009    30077\n",
       "2010    29494\n",
       "2014    29258\n",
       "2015    13514\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will sample certain number of data from yellow taxi data based on this\n",
    "uber_select['year'] = pd.to_datetime(uber['pickup_datetime']).dt.year\n",
    "uber_select['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea58a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4a960f229f9c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uber_select.drop(['Unnamed: 0','key','fare_amount','passenger_count', 'year'], axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "#drop invalid variables and add distance to the dataframe\n",
    "uber_select.drop(['Unnamed: 0','key','fare_amount','passenger_count', 'year'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead69b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-355f9ddcbd83>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uber_select['distance'] = uber_select.apply(lambda x: create_distance(x['pickup_latitude'],\\\n"
     ]
    }
   ],
   "source": [
    "uber_select['distance'] = uber_select.apply(lambda x: create_distance(x['pickup_latitude'],\\\n",
    "                                                                    x['pickup_longitude'],\\\n",
    "                                                                      x['dropoff_latitude'],\\\n",
    "                                                                   x['dropoff_longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849b57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = uber_select.reset_index()\n",
    "uber_data = uber_data.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7520f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data['pickup_datetime'] = pd.to_datetime(uber_data['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facc181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1.681111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>2.454363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0 2015-05-07 19:52:06+00:00        -73.999817        40.738354   \n",
       "1 2009-07-17 20:04:56+00:00        -73.994355        40.728225   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  distance  \n",
       "0         -73.999512         40.723217  1.681111  \n",
       "1         -73.994710         40.750325  2.454363  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dce303",
   "metadata": {},
   "source": [
    "## yellow_taxi data\n",
    "1. Improt taxi_zone.shp and convert geometry data into normal longtitue and lantitude\n",
    "2. Get the url of yellow taxi and collect the data as yearly file and check the data\n",
    "3. Drop invalid variables\n",
    "4. Drop the data with same PULocationID and DOLocationID, merge geographical information and drop data out of given range\n",
    "5. Sample the data accoring to the data amouont in uber\n",
    "6. Add distance to the dataframe\n",
    "7. Because data in 2010 and 2009 have different information in it, we process them seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d101b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import taxi_zone information\n",
    "taxi_zone = gpd.read_file('taxi_zones.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfea1a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  Shape_Leng  Shape_Area            zone  LocationID borough  \\\n",
       "0         1    0.116357    0.000782  Newark Airport           1     EWR   \n",
       "1         2    0.433470    0.004866     Jamaica Bay           2  Queens   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1  MULTIPOLYGON (((1033269.244 172126.008, 103343...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "taxi_zone.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f4afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-770757aa3575>:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone['lon'] = taxi_zone.centroid.x\n",
      "<ipython-input-14-770757aa3575>:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone['lat'] = taxi_zone.centroid.y\n"
     ]
    }
   ],
   "source": [
    "#change geometry information to lantitude and longtitde\n",
    "taxi_zone = taxi_zone.to_crs(4326)\n",
    "taxi_zone['lon'] = taxi_zone.centroid.x  \n",
    "taxi_zone['lat'] = taxi_zone.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d3cf886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((-74.18445 40.69500, -74.18449 40.695...</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>40.691831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...</td>\n",
       "      <td>-73.831299</td>\n",
       "      <td>40.616745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  Shape_Leng  Shape_Area            zone  LocationID borough  \\\n",
       "0         1    0.116357    0.000782  Newark Airport           1     EWR   \n",
       "1         2    0.433470    0.004866     Jamaica Bay           2  Queens   \n",
       "\n",
       "                                            geometry        lon        lat  \n",
       "0  POLYGON ((-74.18445 40.69500, -74.18449 40.695... -74.174000  40.691831  \n",
       "1  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ... -73.831299  40.616745  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_zone.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cd0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url of yellow taxi\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "\n",
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "\n",
    "def find_taxi_parquet_links():\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    yellow_lst = [a['href'] for a in soup.find_all(title = 'Yellow Taxi Trip Records')]\n",
    "    return yellow_lst\n",
    "\n",
    "yellow_lst =  find_taxi_parquet_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db0599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the year_num list according to uber_data \n",
    "year_num = [13514, 29258, 30496, 31531, 31102, 29494, 30077]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c634696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to get and clean data in 2011-2015\n",
    "def get_and_clean_data(n):\n",
    "    file = list(range(1,13))\n",
    "    for i in range(len(file)):\n",
    "        file[i] = pd.read_parquet(yellow_lst[i+12*n-84])\n",
    "    for i in range(len(file)):\n",
    "        file[i].drop(['VendorID','tpep_dropoff_datetime','passenger_count',\\\n",
    "                            'trip_distance','RatecodeID','store_and_fwd_flag',\\\n",
    "                            'payment_type','fare_amount','extra','mta_tax',\\\n",
    "                            'tolls_amount','improvement_surcharge','total_amount',\n",
    "                            'congestion_surcharge','airport_fee'], \\\n",
    "                           axis = 1, inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i].drop(file[i][file[i]['PULocationID'] == file[i]['DOLocationID']].index, inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i].rename(columns = {'PULocationID' : 'LocationID','tpep_pickup_datetime':\\\n",
    "                                 'pickup_datetime'}, inplace = True)\n",
    "        file[i] = pd.merge(file[i], taxi_zone[['LocationID','lon','lat']], how = 'left', on = 'LocationID')\n",
    "        file[i].drop(['LocationID'], axis = 1, inplace = True)\n",
    "        file[i].rename(columns = {'DOLocationID' : 'LocationID','lon':'pickup_longitude', 'lat':\\\n",
    "                                       'pickup_latitude'}, inplace = True)\n",
    "        file[i] = pd.merge(file[i], taxi_zone[['LocationID','lon','lat']], how = 'left', on = 'LocationID')\n",
    "        file[i].drop(['LocationID'], axis = 1, inplace = True)\n",
    "        file[i].rename(columns = {'lon':'dropoff_longitude', 'lat':'dropoff_latitude'}, inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i] = file[i][(file[i]['pickup_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['pickup_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['pickup_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['pickup_latitude'] <= 40.908524) \\\n",
    "                                &(file[i]['dropoff_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['dropoff_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['dropoff_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['dropoff_latitude'] <= 40.908524)]\n",
    "    file_res = pd.DataFrame()\n",
    "    for i in range(len(file)):\n",
    "        file_res = pd.concat([file_res,file[i].sample(round(year_num[n]/12+1))], axis = 0)\n",
    "    file_res = file_res.sample(year_num[n])\n",
    "    file_res['distance'] = file_res.apply(lambda x: create_distance(x['pickup_latitude'],\\\n",
    "                                                                    x['pickup_longitude'],\\\n",
    "                                                                      x['dropoff_latitude'],\\\n",
    "                                                                   x['dropoff_longitude']), axis=1)\n",
    "    \n",
    "    return file_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58f2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to get and clean data in 2010\n",
    "def get_and_clean_data2(n):\n",
    "    file = list(range(1,13))\n",
    "    for i in range(len(file)):\n",
    "        file[i] = pd.read_parquet(yellow_lst[i+12*n-24])\n",
    "    for i in range(len(file)):\n",
    "        file[i].drop(['vendor_id','dropoff_datetime','passenger_count',\\\n",
    "                            'trip_distance','rate_code','store_and_fwd_flag',\\\n",
    "                            'payment_type','fare_amount','surcharge','mta_tax',\\\n",
    "                            'tolls_amount','total_amount'], axis = 1, inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i]['pickup_datetime'] = pd.to_datetime(file[i]['pickup_datetime'])\n",
    "    for i in range(len(file)):\n",
    "        file[i] = file[i][(file[i]['pickup_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['pickup_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['pickup_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['pickup_latitude'] <= 40.908524) \\\n",
    "                                &(file[i]['dropoff_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['dropoff_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['dropoff_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['dropoff_latitude'] <= 40.908524)]\n",
    "    file_res = pd.DataFrame()\n",
    "    for i in range(len(file)):\n",
    "        file_res = pd.concat([file_res,file[i].sample(round(year_num[n+5]/12+1))], axis = 0)\n",
    "    file_res = file_res.sample(year_num[n+5])\n",
    "    file_res['distance'] = file_res.apply(lambda x: create_distance(x['pickup_latitude'],\\\n",
    "                                                                    x['pickup_longitude'],\\\n",
    "                                                                      x['dropoff_latitude'],\\\n",
    "                                                                   x['dropoff_longitude']), axis=1)\n",
    "    \n",
    "    return file_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a455ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to get and clean data in 2009\n",
    "def get_and_clean_data3(n):\n",
    "    file = list(range(1,13))\n",
    "    for i in range(len(file)):\n",
    "        file[i] = pd.read_parquet(yellow_lst[i+12*n-12])\n",
    "    for i in range(len(file)):\n",
    "        file[i].drop(['vendor_name','Trip_Dropoff_DateTime','Passenger_Count',\\\n",
    "                            'Trip_Distance','Rate_Code','store_and_forward',\\\n",
    "                            'Payment_Type','Fare_Amt','surcharge','mta_tax',\\\n",
    "                            'Tolls_Amt','Total_Amt'], axis = 1, inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i].rename(columns = {'Trip_Pickup_DateTime' : 'pickup_datetime'\\\n",
    "                       ,'Start_Lon' : 'pickup_longitude', 'Start_Lat' : 'pickup_latitude',\\\n",
    "                       'End_Lon' : 'dropoff_longitude','End_Lat' : 'dropoff_latitude','Tip_Amt':'tip_amount'},\\\n",
    "                       inplace = True)\n",
    "    for i in range(len(file)):\n",
    "        file[i]['pickup_datetime'] = pd.to_datetime(file[i]['pickup_datetime'])\n",
    "    for i in range(len(file)):\n",
    "        file[i] = file[i][(file[i]['pickup_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['pickup_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['pickup_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['pickup_latitude'] <= 40.908524) \\\n",
    "                                &(file[i]['dropoff_longitude'] >= -74.242330) \\\n",
    "                                & (file[i]['dropoff_longitude'] <= -73.717047) \\\n",
    "                                & (file[i]['dropoff_latitude'] >= 40.560445) \\\n",
    "                                & (file[i]['dropoff_latitude'] <= 40.908524)]\n",
    "    file_res = pd.DataFrame()\n",
    "    for i in range(len(file)):\n",
    "        file_res = pd.concat([file_res,file[i].sample(round(year_num[n+6]/12+1))], axis = 0)\n",
    "    file_res = file_res.sample(year_num[n+6])\n",
    "    file_res['distance'] = file_res.apply(lambda x: create_distance(x['pickup_latitude'],\\\n",
    "                                                                    x['pickup_longitude'],\\\n",
    "                                                                      x['dropoff_latitude'],\\\n",
    "                                                                   x['dropoff_longitude']), axis=1)\n",
    "    \n",
    "    return file_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4728092c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-01ed6bcc4b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile2015_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-fffb3bd20888>\u001b[0m in \u001b[0;36mget_and_clean_data\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         file[i].drop(['VendorID','tpep_dropoff_datetime','passenger_count',\\\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         return IOArgs(\n\u001b[1;32m    370\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file2015_res = get_and_clean_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6535800",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2014_res = get_and_clean_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2013_res = get_and_clean_data(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eab910",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2012_res = get_and_clean_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2011_res = get_and_clean_data(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2010_res = get_and_clean_data2(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395ab84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2009_res = get_and_clean_data3(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d191289",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi = pd.DataFrame()\n",
    "for i in [file2015_res, file2014_res, file2013_res,file2012_res,file2011_res,file2010_res,file2009_res]:\n",
    "    yellow_taxi = pd.concat([yellow_taxi,i], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b16667",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi = yellow_taxi.reset_index()\n",
    "yellow_taxi = yellow_taxi.drop(['index'], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154119d8",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "1. Define functions for cleaning and processing weather data\n",
    "2. Apply the functions for every year data to get yearly dataframes\n",
    "3. Concatenate every year's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This delete_str() function is for processing precipitation data\n",
    "#Because precipitation is in string type and T denote trace amount,\n",
    "#We use an relatively small number to replace the T in order for further number calculation.\n",
    "#x[:4] because there are wrongly number with \"0.003s\", the s we assume is a typo\n",
    "def delete_str(x):\n",
    "    if x == None:\n",
    "        return x\n",
    "    elif x == 'T':\n",
    "        return 0.0001\n",
    "    elif type(x) == float:\n",
    "        return x\n",
    "    else:\n",
    "        return x[:4]\n",
    "    \n",
    "#This reset_drop() function is for dataframe's column normalize\n",
    "#Because we have to concatenate all the dataframes at last which need same column name and index\n",
    "def reset_drop(x):\n",
    "    x = x.reset_index()\n",
    "    x = x.drop(['index'], axis = 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Weather_transfer()function is used for processing yearly csv file\n",
    "#into hourly weather and calculating daily weather\n",
    "#Please note that calculating daily weather is specific for\n",
    "#2009-2012's data, because these data do not have daily summery(Their \"SOD\" row count is much smaller than 365)\n",
    "#For other 2013-2015's data, we only need to fetch the \"SOD\" report which is for daily\n",
    "#(we have test that \"SOD\" rows for 2013 to 2015 all have 365 entries, which we assume is sufficient)\n",
    "def Weather_transfer(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    df = df.loc[df['REPORT_TYPE'] != 'SOD  ']\n",
    "    df = df.loc[df['REPORT_TYPE'] != 'SOM  ']\n",
    "    \n",
    "    df_h = df[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    \n",
    "    df_h['HourlyPrecipitation'] = df_h['HourlyPrecipitation'].apply(delete_str)\n",
    "\n",
    "    df_h['DATE_parsed'] = pd.to_datetime(df_h['DATE'], format = \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    #df_h['HourlyPrecipitation'] = df_h['HourlyPrecipitation'].replace(['T'], 1e-6)\n",
    "    df_h['HourlyPrecipitation'] = df_h['HourlyPrecipitation'].astype(float)\n",
    "    \n",
    "    df_h['HourlyWindSpeed'] = df_h['HourlyWindSpeed'].fillna(0)\n",
    "    df_h['HourlyPrecipitation'] = df_h['HourlyPrecipitation'].fillna(0)\n",
    "\n",
    "    df_h['DATE_day'] = pd.to_datetime(df_h['DATE'], format = \"%Y-%m-%d\").dt.date\n",
    "    group = df_h.groupby('DATE_day')\n",
    "    df_d = group.mean()\n",
    "    df_d.rename(columns={\"HourlyPrecipitation\": \"DailyPrecipitation\", \"HourlyWindSpeed\": \"DailyAverageWindSpeed\"}, inplace = True)\n",
    "    df_d['DailyPrecipitation'] = df_d['DailyPrecipitation'].fillna(0)\n",
    "    #df_h['HourlyPrecipitation'] = df_h['HourlyPrecipitation'].fillna(0)\n",
    "    \n",
    "    \n",
    "    df_h['DATE'] = df_h['DATE_parsed']\n",
    "    df_h.drop(['DATE_parsed','DATE_day'], axis = 1, inplace = True)\n",
    "    \n",
    "    df_d.reset_index(inplace=True)\n",
    "\n",
    "    list_df = [df_h, df_d]\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This get_daily() function is specific for year from 2013 to 2015\n",
    "#As their SOD daily summery is compelete and reliable\n",
    "def get_daily(csv_file):\n",
    "    weather_df = pd.read_csv(csv_file)\n",
    "    df_daily = weather_df.loc[weather_df['REPORT_TYPE'] == 'SOD  '][['DATE', 'DailyPrecipitation', 'DailyAverageWindSpeed']]\n",
    "    df_daily['DailyPrecipitation'] = df_daily['DailyPrecipitation'].apply(delete_str)\n",
    "    df_daily['DailyPrecipitation'] = df_daily['DailyPrecipitation'].astype(float)\n",
    "    df_daily['DailyAverageWindSpeed'] = df_daily['DailyAverageWindSpeed'].astype(float)\n",
    "    \n",
    "    df_daily['DailyAverageWindSpeed'] = df_daily['DailyAverageWindSpeed'].fillna(0)\n",
    "    \n",
    "    df_daily['Date'] = pd.to_datetime(df_daily['DATE'], format = \"%Y-%m-%dT%H:%M:%S\").dt.date\n",
    "    df_daily['DATE'] = df_daily['Date']\n",
    "    df_daily = df_daily.drop(['Date'], axis = 1)\n",
    "    df_daily.rename({'DATE' : 'DATE_day'}, axis = 1, inplace = True)\n",
    "    #df_daily.set_index(['DATE_day'], inplace = True)\n",
    "    df_daily.reindex(columns = ['DATE_day', 'DailyPrecipitation', 'DailyAverageWindSpeed'])\n",
    "    \n",
    "    df_daily.reset_index(inplace=True)\n",
    "    df_daily = df_daily.drop(['index'], axis = 1)\n",
    "    return df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616feb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This get_sun function is for getting the sunrise and sunset time for everyday of 2009 to 2015\n",
    "def get_sun(csv_file):\n",
    "    weather_df_09 = pd.read_csv(csv_file)\n",
    "    df = weather_df_09.loc[weather_df_09['REPORT_TYPE'] == 'SOD  '][['DATE', 'Sunrise', 'Sunset']]\n",
    "    df['Sunrise'] = '0' + df['Sunrise'].astype(int).astype(str) \n",
    "    df['Sunset'] = df['Sunset'].astype(int).astype(str)\n",
    "    df['DATE_day'] = pd.to_datetime(weather_df_09['DATE'], format = \"%Y-%m-%d\").dt.date\n",
    "    df['DATE_Sunrise_str'] = df['DATE_day'].astype(str) + '0' + df['Sunrise'].astype(int).astype(str)\n",
    "    df['DATE_Sunset_str'] = df['DATE_day'].astype(str) + df['Sunset'].astype(int).astype(str) \n",
    "    df['Sunrise_Time'] = pd.to_datetime(df['DATE_Sunrise_str'], format = \"%Y-%m-%d%H%M\")\n",
    "    df['Sunset_Time'] = pd.to_datetime(df['DATE_Sunset_str'], format = \"%Y-%m-%d%H%M\")\n",
    "    df_sun = df[['Sunrise_Time', 'Sunset_Time']]\n",
    "    \n",
    "    df_sun.reset_index(inplace=True)\n",
    "    df_sun = df_sun.drop(['index'], axis = 1)\n",
    "    return df_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we apply the functions to get a list of yearly generated dataframes\n",
    "#Finally we stack the df one on another to get 2012-2015 data\n",
    "#Note that Here is for hourly and daily weather\n",
    "ls_09 = Weather_transfer('2009_weather.csv')\n",
    "ls_10 = Weather_transfer('2010_weather.csv')\n",
    "ls_11 = Weather_transfer('2011_weather.csv')\n",
    "ls_12 = Weather_transfer('2012_weather.csv')\n",
    "ls_13 = [Weather_transfer('2013_weather.csv')[0], get_daily('2013_weather.csv')]\n",
    "ls_14 = [Weather_transfer('2014_weather.csv')[0], get_daily('2014_weather.csv')]\n",
    "ls_15 = [Weather_transfer('2015_weather.csv')[0], get_daily('2015_weather.csv')]\n",
    "name_lst = [ls_09, ls_10, ls_11, ls_12, ls_13, ls_14, ls_15]\n",
    "weather_hour = pd.DataFrame()\n",
    "weather_day = pd.DataFrame()\n",
    "for i in range(7):\n",
    "    weather_hour = pd.concat([weather_hour,name_lst[i][0]], axis = 0)\n",
    "    weather_day = pd.concat([weather_day,name_lst[i][1]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db680887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same process as before but different function\n",
    "#Note that Here is only for daily sunset and sunrise\n",
    "sun_09 = get_sun('2009_weather.csv')\n",
    "sun_10 = get_sun('2010_weather.csv')\n",
    "sun_11 = get_sun('2011_weather.csv')\n",
    "sun_12 = get_sun('2012_weather.csv')\n",
    "sun_13 = get_sun('2013_weather.csv')\n",
    "sun_14 = get_sun('2014_weather.csv')\n",
    "sun_15 = get_sun('2015_weather.csv')\n",
    "sun_lst = [sun_09, sun_10, sun_11, sun_12, sun_13, sun_14, sun_15]\n",
    "sun_time = pd.DataFrame()\n",
    "for i in range(7):\n",
    "    sun_time = pd.concat([sun_time,sun_lst[i]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_hour = reset_drop(weather_hour)\n",
    "weather_day = reset_drop(weather_day)\n",
    "sun_time = reset_drop(sun_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c8095",
   "metadata": {},
   "source": [
    "## Part2 Create SQL database\n",
    "create a database conytaining 5 tables (4 required table and 1 table for sunset and sunrise data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMAND = [\"\"\"\n",
    "DROP TABLE IF EXISTS taxi_trips\n",
    "\"\"\",\"\"\"\n",
    "DROP TABLE IF EXISTS uber_trips\"\"\",\n",
    "\"\"\"\n",
    "DROP TABLE IF EXISTS hourly_weather\"\"\",\n",
    "\"\"\"\n",
    "DROP TABLE IF EXISTS daily_weater\n",
    "\"\"\"]\n",
    "for i in COMMAND:\n",
    "    engine.execute(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd004bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our 5 tables\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NO EXITS hourly_weather\n",
    "(\n",
    "    id INTERGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NO EXITS daily_weather\n",
    "(\n",
    "    id INTERGER PRIMARY KEY,\n",
    "    DATE_day DATE,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NO EXITS taxi_trips\n",
    "(\n",
    "    id INTERGER PRIMARY KEY,\n",
    "    pickup_datetime DATETIME,\n",
    "    tip_amount FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NO EXITS uber_trips\n",
    "(\n",
    "    id INTERGER PRIMARY KEY,\n",
    "    pickup_datetime FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "SUN_TIME_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NO EXITS sun_time\n",
    "(\n",
    "    id INTERGER PRIMARY KEY,\n",
    "    Sunrise_Time DATETIME,\n",
    "    Sunset_Time DATETIME\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUN_TIME_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786fe0a",
   "metadata": {},
   "source": [
    "## Add data into database\n",
    "add cleaned data to the table we create above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb87249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    add = list(table_to_df_dict.items())\n",
    "    for i in range(len(add)):\n",
    "        add[i][1].to_sql(add[i][0],engine)\n",
    "        print('added', add[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": yellow_taxi,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": weather_hour,\n",
    "    \"daily_weather\": weather_day,\n",
    "    \"sun_time\":sun_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9164e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe26ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data\n",
    "pd.read_sql('daily_weather', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('taxi_trips', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53531a",
   "metadata": {},
   "source": [
    "## Part3 Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee372d47",
   "metadata": {},
   "source": [
    "### Q1\n",
    "For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT COUNT(*) AS trips, strftime('%H', pickup_datetime) AS hour\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime <'2015-07-01 00:00:00'\n",
    "GROUP BY hour\n",
    "ORDER BY trips\n",
    "\"\"\"\n",
    "result1 = engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2831909",
   "metadata": {},
   "source": [
    "### Q2\n",
    "For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6858dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
